---
title: "Analysis of Crime Data in Los Angeles 2021-2024"
author: "Sarah Deussing & Imogen Meers"
date: "2024-09-20"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation
Our group is working with Los Angeles crime data from 2021-2024. The dataset is divided into region and lists the date, time, and type of crime. This is an interesting problem because of its practical implications - after identification of areas with high crime, the Los Angeles police department can place more officers at such locations (and at certain identified times).

This problem has been attempted by other people on Kaggle although mostly in an exploratory sense. Many projects on this dataset look at victim type and crime rate by hour, which are both features we will adapt and use for our analysis by including victim age in our derived variables and predicting hours of crime based on other features.

One analysis by Navin Pal Singh on Kaggle, looked into distribution of crimes both geospatial, by time and by crime type. The output of his analysis was mostly visualizations including a bar chart of crime codes, a 3D plot of number of crimes by time and area, and a map plot of the areas. In general, his visualizations were hard to read with many variables and points overlapping on the axis and in the plot. We will look into using a similar technique of maps and bars to see distribution for our exploratory analysis, before choosing how to group our data for our models.

Another analysis by Ronaldo Pangarego, that was published on Medium did a deep dive into all the variables in the LAPD crime data set. He found years, areas and days of the week that had the most crime. As well as distribution of crime type, weapon type and victim. Pangarego found that the most common crime type was "vehicle-stolen" and weapon was "strong-arm" so it will be interesting to see going forward if these categories have more predictive power in any of our models.

Our analysis will be unique in that we are going to introduce several derived variables (explained in the Data Overview section below). We will also be using machine learning models to predict the type and time of crime, rather than producing mostly descriptive outputs.

## Problem Framing
Our proposed solution can be divided into 2 parts.

Part 1:
Our plan is to identify districts of Los Angeles and times of the day that are "high crime." We will do so via clustering methods, and we will account for the severity of the crime and vulnerability of victim in our analysis.

Part 2:
Of the districts identified as "high-crime" in Part 1, we will then predict the next time and type of crime (violent/non-violent) to occur.

## Data Overview
The dataset contains columns that give the date, time, and location (area code, area name, and exact location) of the reported crime. There are also columns for the use of weapons (yes/no), the crime code, and the longitude/latitude coordinates. 

We will create the following __row-wise derived variables__:

1. __Severity__ index that evaluates crime codes and assigns each crime a ranking of how severe the offense was

  - This information will be useful in deciding if an area is "high-crime." For example, if two areas have similar crime rates/totals, the crime severity will differentiate these two areas. 
  
  - By having a final, combined severity it also allows us to evaluate whether two minor crimes are worse than one major crime.
  
  - Upon research, we found the Uniform Crime Reporting User Manual, which gave a break down of how offenses should be classified. It described that a lower crime code indicated a more violent crime and that each crime is either part 1, a crime against person or property, or a less severe part 2 crime, which is any other crime. Using this information, we ordered each crime firstly by part, then by crime code and gave a severity index from 140 (most severe) to 1 to (least severe).
  
  a. Severity for Crime Code 1
  
  b. Severity for Crime Code 2
  
  c. Severity for Crime Code 3
  
  d. Final Severity, which is a weighted severity that takes into account each crime code. Crime code 1 is the primary offense so is 75% of final, crime code 2 is the secondary offesnse so is 25% of final and crime code 3 is the tertiary offense so is 10% of final
  
  
2. __Vulnerability__ index that evaluates age and gender of the victim and assigns a number to decide how vulnerable they would be
  - This is be calculated by 3 points for female and + 1 point for every year < 18 or > 60
  - The introduction of this variable eliminates some of our project's reproducability as it is subjective how much being female or young/old affects your vulnerability.
  
   
3. An indicator variable to describe gun (1) or not (0)

4. An indicator variable to describe blade (1) or not (0)

5. An indicator variable to describe Adult Arrest (1) or not

6. An indicator variable to describe Juv Arrest (1) or not (0)


From our row-wise derived variables, we will create __district-wise derived variables__ for use in clustering:

1. Mean Severity

2. Mean Vulnerability

3. Number of Gun Crimes

4. Number of Blade Crimes

5. Number of Adult Arrests

6. Number of Juvenile Arrests

7. Most common weapon type

8. Most common premises at which the crime occured (e.g. does District X have lots of crimes in family homes?)

9. Mean latitude

10. Mean longitude


The dataset can be accessed at:
https://catalog.data.gov/dataset/crime-data-from-2020-to-present

## Implementation

#### Initial Data Cleaning

```{r Read in Data}
library(readr)
crime <- read_csv("Crime_Data_from_2020_to_Present.csv")
head(crime)
summary(crime)
```


```{r Clean Column Names: Eliminate Spaces}
#colnames(crime)
new_colnames <- c("DR_NO", "Date_Rptd", "Date_Occurred", "Time_Occurred", "Area",
                  "Area_Name","District_Num", "Part_1_2", "Crime_Code", "Crime_Code_Desc",
                  "Mocode", "Vict_Age", "Vict_Sex", "Vict_Descent", "Premis_Code",
                  "Premis_Desc", "Weapon_Used_Code", "Weapon_Desc", "Status", "Status_Desc",
                  "Crm_Cd1", "Crm_Cd2", "Crm_Cd3", "Crm_Cd4", "Location", "Cross_Street",
                  "LAT", "LON")
colnames(crime) <- new_colnames

```

#### Exploratory Analysis

Number of Crimes by Area Code
```{r Number of Crimes by Area Code}
library(ggplot2)
by_area <- ggplot(crime, aes(x=Area)) + geom_bar() +
  labs(title = "Number of Crimes by Area Code",
       x = "Area Code",
       y = "Number of Crimes") +
  theme_minimal()
by_area
```

The areas with the most crime in the dataset are: 1, 3, 12, and 14.

Number of Crimes by Victim Age/Sex
```{r Number of Crimes by Victim Age and Sex}
filtered <-  crime[crime$Vict_Age > 0 & ((crime$Vict_Sex == 'F' | crime$Vict_Sex == 'M') & !is.na(crime$Vict_Sex)),]

by_age <- ggplot(filtered, aes(x=Vict_Age)) + 
  geom_histogram(fill = "grey", color = "black") +
  geom_histogram(data = filtered[filtered$Vict_Age > 0 & filtered$Vict_Age < 18 | filtered$Vict_Age >= 60,], fill = "red", color = 'black', ) + 
  labs(title = "Vulnerability- Number of Crimes by Victim Age and Sex",
       x = "Victim Age",
       y = "Number of Crimes") +
  theme_minimal() +
  annotate("text", x = 80, y = 30000, label = "Vunerable Population\n Age < 18 or Age > 60", color = "red", size = 3) +
  facet_wrap(~Vict_Sex)
by_age
```

The distribution of victim ages is relatively normal and similar between male and female with the most crimes being committed against males and females aged 25 to 50. From the area of each histogram you can see there is a roughly equal number of crimes committed against males and females. 

Severity of Crimes (Area)
```{r Severity of Crimes by Area}
library(dplyr)
library(ggmap)


#Dataset including indicator variables for armed and arrested
indicators <- crime
indicators$Arrest <- ifelse((indicators$Status == 'JA' | indicators$Status == 'AA') & !is.na(indicators$Status), 1, 0)
indicators$Armed <- ifelse(!is.na(indicators$Weapon_Used_Code), 1, 0)


#Summary Table by Area
summary_by_area <- indicators[indicators$LON != 0 & indicators$ LAT !=0,] %>%
  group_by(Area) %>%
  summarise(
    avg_lat = mean(LAT),
    avg_lon = mean(LON),
    Percent_Arrests = (sum(Arrest) / n()) * 100,
    Percent_Armed = (sum(Armed) / n()) * 100
  )

```

Crimes on Los Angeles Map
```{r crimes on LA map}
register_stadiamaps("9e07144d-cdab-48f9-a35d-298e1bbece2e")

# Define the bounding box for Los Angeles
bbox <- c(left = -118.6682, bottom = 33.7037, right = -118, top = 34.3373)

# Get the map
la_map <- get_stadiamap(bbox = bbox, zoom = 12, maptype = "stamen_toner")


# Plot the map
armed_map <- ggmap(la_map) +
  geom_point(data = indicators, aes(x = LON, y = LAT, color = as.factor(Armed), alpha = 0.05)) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  scale_color_manual(
    values = c('1' = "red", '0' = "blue"),
    labels = c("Unarmed", "Armed")
  ) + 
  labs(
    title = "Map of LA showing all Crimes by Weapon Usage",
    color = "Weapon Usage"
  )+
  guides(alpha = "none")

armed_map
```

This shows every crime in out data set coloured by armed or not. It is quite difficult to tell as there are so many crimes that overlap but you can see some areas where there are more concentrated crimes with weapons, such as the centre.

```{r arrest map}
arrest_map <- ggmap(la_map) +
  geom_point(data = summary_by_area, aes(x = avg_lon, y = avg_lat, color = Percent_Arrests, size = 3))+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing % of Crimes ending in Arrest by Area"
  ) +
  guides(size = "none") +
  scale_color_gradient(low = "blue", high = "red") 
#arrest_map


armed_map1 <- ggmap(la_map) +
  geom_point(data = summary_by_area, aes(x = avg_lon, y = avg_lat, color = Percent_Armed, size = 3))+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing % of Armed Crimes by Area"
  ) +
  guides(size = "none") +
  scale_color_gradient(low = "blue", high = "red") 
#armed_map1

library(gridExtra)
grid.arrange(arrest_map, armed_map1, nrow=1)
```

From this map, we can see there are areas particularly towards the north and centre where there are a higher percentage of armed crimes and crimes that lead to arrests. This indicates that both armed and arrested may be good variables to help cluster our areas into high-low crime. 

#### Creating Derived Variables

Vulnerability Metric
```{r Create Vulnerability Metric}
crime$Vict_Age <- ifelse(crime$Vict_Age <= 0, NA, crime$Vict_Age)
crime$Vulnerability <- 0

 createVuln <- function(row){
   x <-  0
  if(!is.na(row['Vict_Sex'])){ #adding points for F
    if (row['Vict_Sex'] == "F"){
      x  <- x + 3
    }
  }
  
  if(!is.na(row['Vict_Age'])){ #adding points for Age
    if (as.numeric(row['Vict_Age']) < 18){
      x  <- x + 18 - as.numeric(row['Vict_Age'])
    }
    
    else if (as.numeric(row['Vict_Age']) > 60){
      x <- x + as.numeric(row['Vict_Age']) - 60
    }
  }
   return(x)
}
crime$Vulnerability <- apply(crime, 1, createVuln)
```

Severity Metric
```{r Create Severity Metric}
library(dplyr)
#Ranked our crimes in terms of hierarchy and if part 1 or 2 offense
rank_code <- crime %>% select(Crime_Code_Desc, Crime_Code, Part_1_2)%>% distinct(Crime_Code_Desc,Crime_Code, Part_1_2) %>%  arrange(desc(Part_1_2), desc(Crime_Code)) %>%  mutate(Severity = row_number())

#Join on overall CrimeCode --> Do this for Crime Code 1, 2, 3, 4
rank_code <- rank_code %>% select(Severity, Crime_Code)

#Crime Code 1
crime <- left_join(crime, rank_code, by = "Crime_Code")
crime <- crime %>%
  rename(Severity_1 = Severity)

#Crime Code 2
rank_code <- rank_code %>%
  rename(Crm_Cd2 = Crime_Code)
crime <- left_join(crime, rank_code, by = "Crm_Cd2")
crime <- crime %>%
  rename(Severity_2 = Severity)

# Crime Code 3
rank_code <- rank_code %>%
  rename(Crm_Cd3 = Crm_Cd2)
crime <- left_join(crime, rank_code, by = "Crm_Cd3")
crime <- crime %>%
  rename(Severity_3 = Severity)

# Crime Code 4 is all NA, so remove it.
crime <- crime %>%
  select(-c(Crm_Cd4))

# Calculate final severity
#0.75*CRM1 +0.25*CRM2 + 0.1*CRM3 = FINAL SEVERITY
crime$Severity_2[is.na(crime$Severity_2)] <- 0
crime$Severity_3[is.na(crime$Severity_3)] <- 0
crime <- crime %>%
  mutate(Final_Severity = (0.75*Severity_1) + 
  (0.25*Severity_2) + 
  (0.1*Severity_3))
```

Weapons & Arrests
```{r Weapons and arrests}
# Arrests --> count AA (adult arrest) and JA (juvenile arrest)
crime$Adult_Arrest <- ifelse(crime$Status == "AA", 1, 0)
crime$Juv_Arrest <- ifelse(crime$Status == "JA", 1, 0)

# Weapons --> gun (starts with 1), blade (starts with 2)
crime$Weapon_Used_Code <- as.character(crime$Weapon_Used_Code)
crime$Gun <- ifelse(substr(crime$Weapon_Used_Code,1,1) == "1", 1, 0)
crime$Gun <- ifelse(is.na(crime$Gun), 0, crime$Gun)
crime$Blade <- ifelse(substr(crime$Weapon_Used_Code,1,1) == "2", 1, 0)
crime$Blade <- ifelse(is.na(crime$Blade), 0, crime$Blade)
```

Final Crime Dataframe
```{r Create Final Dataframe}
crime <- crime %>%
  select(-c(DR_NO, Date_Rptd, Part_1_2, Mocode, Location, Cross_Street, Vict_Age, Vict_Sex,
            Crm_Cd1, Crm_Cd2, Crm_Cd3, Vict_Descent, Severity_1, Severity_2, Severity_3))
#write.csv(crime, 'crime_final.csv')
```

Group Crimes by District
```{r Group by District}
library(DescTools)
crime$Weapon_Used_Code <- as.numeric(crime$Weapon_Used_Code)

districts <- crime %>% group_by(District_Num) %>%
  summarize(Most_Common_Premis = Mode(Premis_Code, na.rm=TRUE),
            Most_Common_Crime = Mode(Crime_Code, na.rm=TRUE),
            Avg_Vulnerability = mean(Vulnerability),
            Avg_Severity = mean(Final_Severity),
            Avg_LON = mean(LON),
            Avg_LAT = mean(LAT),
            Num_Adult_Arrest = sum(Adult_Arrest),
            Num_Juv_Arrest = sum(Juv_Arrest),
            Num_Gun = sum(Gun),
            Num_Blade = sum(Blade),
            Most_Common_Weapon = Mode(Weapon_Used_Code, na.rm=TRUE))

#write.csv(districts, 'districts.csv')
```

Read in dataframe (from above)
```{r, eval = FALSE}
#districts <- read.csv('districts.csv')
#crime <- read.csv('crime_final.csv')
```

## Methods for Analysis
Methods (0.5 - 1 pages) - Provide a one to two paragraph description of the method used, its applicability to/suitability for the problem along with the methods strengths and limitations. I am looking to see your understanding of the method here.

We will be using two methods for analysis.

1. PAM K-Means Clustering:
  Clustering is an unsupervised learning method. This model partitions the data into a chosen 'k' number of clusters. It does so by clustering around the 'mediods.' In this iterative process, 'k' random entries are chosen as the mediods, and then every other entry is assigned to the closest mediod, making distinct clusters. Then, the entry within each cluster that has the lowest average distance to all other entries is assigned to be the new mediod. This process is repeated until the mediods don't change - then, the clusters have been created. 
  This is similar to the k-means clustering done in class with one key difference. K-means clustering selects the 'centroid' as the center of the cluster (a point with the smallest distance from everything in the cluster, but doesn't necessarily need to be an entry). PAM clustering uses centroids, which are entries themselves.
  PAM Clustering was chosen because it can better handle categorical variables in its clustering process. One downside is that it has a slower runtime than traditional k-means. It is suitable for our analysis because we have almost 1 million entries and we want to analyze only a few "high-crime" areas. Clustering will identify these areas.
  

2. XGBoost:
  XGBoost is a supervised learning model. It is a very high-performing model that can find both linear and non-linear patterns between the predictions and the outcome variable using an iterative decision tree process. Because our dataset is large and we have many predictors, we can expect slower runtimes for this model and must be cautious about overfitting. We will run two different xgboost models: (1) time of the day, and (2) violence. 
  (1) The Time of the Day model will use the "multi:softprob" objective because time of the day has more than 2 classifications (i.e. is not binary). The model will output a probability distribution of all classes for each entry in the data. We will select the class with the highest probability to be the predicted class. We can then look at the balanced accuracy for each class to see how our model performs.
  (2) The Violent model will use the "binary:logistic" objective because a crime is either non-violent (0) or violent (1). The binary model will output a probability for each entry, and then we select a threshold value  between 0 and 1. Probabilities below the threshold become a predicted 0, and probabilies above the threshold become a predicted 1. We can then look at the balanced accuracy, sensitivity, and specificity for our model using a confusion matrix. Balanced accuracy is the average of our sensitivity and specificity scores, and accounts for class imbalance (which regular accuracy does not). Sensitivity is the "true positive rate" - the number of positive classes that are predicted as positive. Specificity is the "true negative rate" - the number of negative classes that are predicted as negative. 
  For both models, we can tune the number of rounds to find the best iteraction, and (after prediction), extract the variables that were the most important as predictors.

#### PAM K-Means Clustering
```{r PAM K-Means CLustering}
library(cluster)
library(tibble)
library(ggplot2)

d <-  districts
clust_data <- ungroup(d) %>% select(-c(Avg_LON, Avg_LAT, District_Num))

#factor non numeric
clust_data$Most_Common_Premis <- as.factor(clust_data$Most_Common_Premis)
clust_data$Most_Common_Crime <- as.factor(clust_data$Most_Common_Crime)
clust_data$Most_Common_Weapon <- as.factor(clust_data$Most_Common_Weapon)


#scale numeric cols
clust_data <- clust_data %>%
    mutate(across(where(is.numeric), scale))


clust_data <- clust_data  %>% 
  mutate(across(where(is.matrix), as.numeric))


gower_dist <- 
  clust_data %>% daisy(metric = "gower")

sil_width <- c(NA)
for (i in 2:20){
  pam_fit <- pam(gower_dist, diss =TRUE, k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}


sil_width %>% 
  as_tibble() %>% 
   rowid_to_column() %>% 
   filter(rowid %in% c(2:20)) %>% 
   ggplot(aes(rowid, value)) +
   geom_line(colour  = 'black', size = 0.7) +
   geom_point(colour = 'black', size = 1.3) +
   theme_minimal() +
   labs(title = 'Silhouette Widths of k-medoid Clusters',
        x     = "Number of clusters",
        y     = 'Silhouette Width') +
   theme(plot.title = element_text(hjust = 0.5))

#sil_width %>% 
 # as_tibble() %>% 
  # rowid_to_column()
```
Looking at this graph, both 7 and 13 are local maximums so we will look at the clustering for both of these.

```{r}
library(Rtsne)

pam_fit7 <- pam(gower_dist, diss =TRUE, k = 7)
pam_fit13 <- pam(gower_dist, diss =TRUE, k = 13)



tsne_obj7 <- Rtsne(gower_dist, is_distance = TRUE)
tsne_obj7$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit7$clustering)) %>% 
  # plot
  ggplot(aes(x = X, y = Y, colour = cluster)) +
  geom_point()  +
  theme_light() +
  labs(title     = 't-SNE 2D Projections of 7-medoid Clusters')  +
  theme(plot.title = element_text(hjust = 0.5))



tsne_obj13 <- Rtsne(gower_dist, is_distance = TRUE)
tsne_obj13$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit13$clustering)) %>% 
  # plot
  ggplot(aes(x = X, y = Y, colour = cluster)) +
  geom_point()  +
  theme_light() +
  labs(title     = 't-SNE 2D Projections of 13-medoid Clusters')  +
  theme(plot.title = element_text(hjust = 0.5))
```

The clusters when k = 13 are more distinct than when k = 7, so we will use 13 clusters for further analysis.
```{r}
library(tidyr)
# Extract clusters
clusters_13 <- pam_fit13$cluster
# Extract centers
#centers_13 <- pam_fit13$medoids
temp <- cbind.data.frame(clust_data, clusters_13)
centers <- temp %>% 
  group_by(clusters_13) %>%
  summarise()

res_mat <- as.data.frame(matrix(NA, nrow = length(unique(clusters_13)), ncol = 6))

for(i in 1:nrow(res_mat)){
  res_mat[i,] <- colMeans(clust_data[which(clusters_13 == i), c(3:8) ], na.rm  =TRUE)
}

names(res_mat) <- names(clust_data)[c(3:8)]
# Create cluster vector
cluster <- c(1:13)
# Join cluster vector and centers
center_df <- data.frame(cluster, res_mat)

# Reshape the data
center_reshape <- gather(center_df, features, values, Avg_Vulnerability:Num_Blade)
# View result
head(center_reshape)

# Create plot
g_heat_2 <- ggplot(data = center_reshape, # Set dataset
                   aes(x = features, y = cluster, fill = values)) + # Set aesthetics
  scale_y_continuous(breaks = seq(1, 13, by = 1)) + # Set y axis breaks
  geom_tile() + # Set geom tile for heatmap
  coord_equal() +  # Set coord equal 
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =0, # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  coord_flip() # Rotate plot
# Generate plot
g_heat_2
```

We see several clusters that are distinct in our heatmap.
  - Group 10: Lots of guns/blade/arrests
  - Group 11: High vulnerability and severity, low guns/blades/arrests
  - Group 4: High vulnerability
  - Group 1: Safest (low severity/vulnerability, guns/blades/arrests)

Assign cluster numbers back for mapping purposes.
```{r}
d_details <- d %>% select(District_Num, Avg_LON, Avg_LAT)
length(clusters_13)
named_clust <- cbind(d_details, clusters_13) %>% rename( "Cluster_Num" = `...4`)

categoric_added <- cbind(named_clust, d[,c(2, 3, 12)])  
categoric_added <- categoric_added[categoric_added$Cluster_Num %in% c(1, 4, 10, 11),]

sum_by_cluster <- categoric_added %>% 
  group_by(Cluster_Num) %>%
  #select(-District_Num) %>%
  mutate(modePrem = Mode(Most_Common_Premis),
         modeCrime = Mode(Most_Common_Crime),
         modeWeapon = Mode(Most_Common_Weapon),
         avgLat = mean(Avg_LAT),
         avgLon = mean(Avg_LON))
```

Let's look at our clusters on a map.
```{r Clusters by Color Map}

cluster_map <- ggmap(la_map) +
  geom_point(data = sum_by_cluster, aes(x = Avg_LON, y = Avg_LAT, color = as.factor(Cluster_Num)), size = 2)+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing our Highlighted Clusters",
    color = "Cluster Number"
  ) +
  guides(size = "none") 
cluster_map
```


#### Prediction

Our clusters of interest are 1, 4, 10, and 11.
  - Cluster 10: lots of guns, blades, and arrests
  - Cluster 11: high vulnerability and severity measures, but low weapon use and arrests
  - Cluster 4: high vulnerability measures
  - Cluster 1: the safest cluster (low vulnerability, severity, weapon use, arrests)

In order to analyze these clusters, we'll assign the cluster number back to the entries in our original crime dataframe and then subset clusters of interest.
```{r Put cluster numbers back}
clust_crime <- crime %>%
  left_join(named_clust, by = "District_Num")
```


We want to analyze trends within 'violent crimes.' For our analysis, we will consider a violent crime one that has a crime code of less than 200 (making it the most severe in the Los Angeles rankings), or uses either a gun or a blade.
```{r Make 'violent crime' variable}
# violent crime = crime code < 200 or uses a blade/gun
clust_crime$violent <- ifelse (((clust_crime$Crime_Code < 200) | (clust_crime$Gun == 1) | (clust_crime$Blade == 1)), 1, 0)
```

Looking further into this violence measure, we can analyze the most violent districts within each "high-crime" cluster. This information could provide the Los Angeles police department with the most high-crime districts within these areas. 
```{r Find most violent districts}
worst10 <- clust_crime %>% 
  filter(Cluster_Num == 10) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst10$District_Num[1] 
# District 1249 is the most "violent" in cluster 10

worst11 <- clust_crime %>% 
  filter(Cluster_Num == 11) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst11$District_Num[1] 
# District 2187 is the most "violent" in cluster 11

worst4 <- clust_crime %>% 
  filter(Cluster_Num == 4) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst4$District_Num[1] 
# District 1846 is the most "violent" in cluster 4

safest1 <- clust_crime %>% 
  filter(Cluster_Num == 1) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(sum_violent) %>%
  select(District_Num) %>%
  head(1)
safest1$District_Num[1] 
# District 1759 is the "safest/least violent" in cluster 1

```


For further analysis, we will look at all the districts within our three highlighted "high-crime" clusters: 4, 10, and 11.

Further setup for our XGBoost models include standardizing time and date of the crime. We will also create a variable for day of the week Sunday (1) - Saturday (7).
```{r Clean Time/Date}
library(lubridate)
clust_crime$hour <- as.numeric(clust_crime$Time_Occurred)  %/% 100

clust_crime$Date <- as.Date(mdy_hms(clust_crime$Date_Occurred))
clust_crime$DayOfWeek <- wday(mdy_hms(clust_crime$Date_Occurred), label = TRUE)
clust_crime$month <- month(mdy_hms(clust_crime$Date_Occurred))
clust_crime$year <- year(mdy_hms(clust_crime$Date_Occurred))


day_map <- c(Sun = 1, Mon = 2, Tues = 3, Wed = 4, Thurs = 5, Fri = 6, Sat = 7)
clust_crime$numDayOfWeek <- day_map[clust_crime$DayOfWeek]
```

```{r Make worst clusters df}
worst_clusts <- clust_crime %>%
  filter(Cluster_Num == 4 | Cluster_Num == 10 | Cluster_Num == 11)

worst_clusts$District_Num <- as.numeric(worst_clusts$District_Num)

# There are 2 rows with NA for Premis Code.
worst_clusts <- worst_clusts[!is.na(worst_clusts$Premis_Code), ]
```

Setup for XGBoost: Premis_Code and Crime_Code are categorical variables, so they need to be made into dummy variables. 
```{r Setup for XGBoost}
#worst_clusts <- fastDummies::dummy_cols(worst_clusts,select_columns = c('Premis_Code', 'District_Num'), remove_selected_columns = TRUE )
dummy_vars <- model.matrix(hour ~ Premis_Code + District_Num - 1, data = worst_clusts)

# Combine into final dataframe for hour
hour_data <- as.data.frame(cbind(worst_clusts$hour, dummy_vars, 
                                 worst_clusts[, c("violent", "Adult_Arrest", "Juv_Arrest", "Gun", "Blade")]))
hour_data <- hour_data %>%
  rename(hour = `worst_clusts$hour`)
```


###### Hour (Time of Day) Models
Our first prediction will be the hour that a crime occurs. Factors will include whether the crime was violent, the crime code, premise code, and whether there were any arrests or weapons involved in the crime.
```{r XGBoost for hour}
library(xgboost)
library(caret)
set.seed(1234567)


vars <- hour_data %>%
  select(-hour)
outcome <- hour_data$hour

train_indices <- sample(1:nrow(hour_data), 0.8 * nrow(hour_data))
train_set <- vars[train_indices, ]
test_set <- vars[-train_indices, ]
train_label <- outcome[train_indices]
test_label <- outcome[-train_indices]

dtrain <- xgb.DMatrix(data = as.matrix(train_set), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_set))

# Train the model
hour_model <- xgb.train(data = dtrain, 
                   nrounds = 100,
                   eta = 0.1,
                   objective = "multi:softmax",
                   num_class = 24,
                   eval_metric = "merror")

predictions <- predict(hour_model, dtest)
pred_and_test <- cbind.data.frame(predictions, test_label)
confusionMatrix(factor(predictions), factor(test_label))
```

This model is too granular to make good predictions. Instead, let's predict time of day in 6 four-hour periods.
  - 6am - 9:59am (morning = 0)
  - 10am - 1:59pm (midday = 1)
  - 2pm - 5:59pm (afternoon = 2)
  - 6pm - 9:59pm (evening = 3)
  - 10pm - 1:59am (late night = 4)
  - 2am - 5:59am (early morning = 5)
  
These periods were assigned to capture 'night time' (10pm - 6am) as sequential periods.
```{r Make time periods}
time_of_day_func <- function(hour) {
  if (hour >= 6 && hour < 10) {
    return(0)  # morning
  } else if (hour >= 10 && hour < 14) {
    return(1)  # midday
  } else if (hour >= 14 && hour < 18) {
    return(2)  # afternoon
  } else if (hour >= 18 && hour < 22) {
    return(3)  # evening
  } else if (hour >= 22 || hour < 2) {
    return(4)  # late night
  } else {
    return(5)  # early morning
  }
}

worst_clusts <- worst_clusts %>%
  mutate(TimeOfDay = sapply(hour, time_of_day_func))
```

We need to do the same dummy variable setup for our time of day model.
```{r TOD setup}
# Combine into final dataframe for time of day
tod_data <- as.data.frame(cbind(worst_clusts$TimeOfDay, dummy_vars, 
                                 worst_clusts[, c("violent", "Adult_Arrest", "Juv_Arrest", "Gun", "Blade")]))
tod_data <- tod_data %>%
  rename(TimeOfDay = `worst_clusts$TimeOfDay`)
```

Now we can run a model to predict a crime's time period of the day.
```{r XGBoost for time period}
vars <- tod_data %>%
  select(-TimeOfDay)
outcome <- tod_data$TimeOfDay

train_indices <- sample(1:nrow(tod_data), 0.8 * nrow(tod_data))
train_set <- vars[train_indices, ]
test_set <- vars[-train_indices, ]
train_label <- outcome[train_indices]
test_label <- outcome[-train_indices]

dtrain <- xgb.DMatrix(data = as.matrix(train_set), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_set))
```

Let's tune the number of rounds for this model.
```{r tune for time period, eval=FALSE}
# Tune number of rounds
tune_nrounds <- xgb.cv(data = dtrain,
              nfold = 5,
              eta = 0.1, 
              nrounds = 1000, 
              early_stopping_rounds = 50,
              verbose = 1, 
              nthread = 1,
              print_every_n = 20, 
              objective = "multi:softprob", 
              num_class = 6,
              eval_metric = "merror")

best_nrounds_tod <- tune_nrounds$best_iteration
best_nrounds_tod
```

Now that we have the best number of iterations, we can train and predict with our model.
```{r}
# Train the model
tod_model <- xgb.train(data = dtrain, 
                   nrounds = best_nrounds_tod, # best iteration
                   eta = 0.1,
                   objective = "multi:softprob",
                   num_class = 6,
                   eval_metric = "merror")

predictions <- predict(tod_model, dtest)

pred_matrix <- matrix(predictions, ncol = 6, byrow = TRUE)
classes <- apply(pred_matrix, 1, which.max) - 1
confusionMatrix(factor(classes), factor(test_label))
```

We see a balanced accuracy of between 0.50 - 0.55 for each class (time of day). 

Let's look at variable importance.
```{r}
# Extract importance
imp_mat <- xgb.importance(model = tod_model)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 7)
```

We see that Premis_Code is the most significant variable in terms of predictors. Let's further analyze the relationship between premis code and time of day.

We can make partial dependence plots (pdp) for our most important variables.
```{r}
library(pdp)

pdp_result <- partial(tod_model, pred.var = "Premis_Code", plot = TRUE, train = as.matrix(train_set),
                      prob=TRUE)
pdp_result
```

We see that premis codes between 800 and 1,000 contribute the most as predictors. 
```{r}
crime %>% select(Premis_Code, Premis_Desc) %>% filter(Premis_Code > 800 & Premis_Code < 1000) %>% distinct() %>% sample_n(30)
```

We see that the Metro System in Los Angeles is the greatest predictor for the hour of crime.

Let's look at the strength of our other predictors as well.
```{r}
library("ggforce")
library("lightgbm")
library("SHAPforxgboost")

shap_long_iris <- shap.prep(xgb_model = tod_model, X_train = as.matrix(train_set))

# Plot SHAP importance
shap.plot.summary(shap_long_iris, scientific = TRUE)
```


###### Violence Model
In our next model, we will predict whether a crime will be violent. To do so, our predictors will include the day of the week, hour of the day, premise code, area, and the vulnerability level of the individual to which the crime is performed.

This will help an officer know if a crime is reported at on x day at x time in x premises with x victim, whether they should be prepared for violence.

First, we can encode hour as a sine/cosine value to make it loop (i.e. 00:00 is after 23:59). We will do the same for the day of the week and month numbers.
```{r Sine/Cos Encoding}
encode <- function(data, idx, max_val){
  print(colnames(data[idx]))
  col_sin <- paste0(colnames(data[idx]), '_sin')
  
  col_cos <- paste0(colnames(data[idx]), '_cos')

  sin_func <- function(x) {sin(2 * pi * as.numeric(x)/max_val)}
  cos_func <- function(x) {cos(2 * pi * as.numeric(x)/max_val)}

  
  data$V1 <-  lapply(data[,idx], sin_func)
  data$V2  <-  lapply(data[,idx], cos_func)
  data <- data %>% rename(!!col_sin := V1, !!col_cos := V2)
  return(data)
}

# Encode Day of Week
dow_col <- which(colnames(worst_clusts) == 'numDayOfWeek')
worst_clusts <- encode(worst_clusts, dow_col, 7)

#Encode hour
tod_col <- which(colnames(worst_clusts) == 'TimeOfDay')
worst_clusts <- encode(worst_clusts, tod_col, 6)

#Encode month
mth_col <- which(colnames(worst_clusts) == 'month')
worst_clusts <- encode(worst_clusts, mth_col, 12)
```

We will run our violent crime model on the same 'worst clusters' as before.

We need to do the same dummy variable conversion as before.
```{r}
# Combine into final dataframe for violence
violent_data <- as.data.frame(cbind(worst_clusts$violent, dummy_vars, 
                                 worst_clusts[, c("year", "numDayOfWeek_sin", "numDayOfWeek_cos", "TimeOfDay_sin", "TimeOfDay_cos", "month_sin", "month_cos", "Vulnerability")]))
violent_data <- violent_data %>%
  rename(violent = `worst_clusts$violent`)
```


```{r Violent crime model}
library(splitstackshape)

vars <- violent_data %>%
  select(-violent)
outcome <- violent_data$violent
xg_data <- cbind(vars, outcome)

# Perform stratified sampling
split_dat <- stratified(xg_data, # Set dataset
                         group = c("outcome", "year", "Premis_Code", "District_Num"),
                         size = 0.2,
                         bothSets = TRUE ) 
 
train_dat <- split_dat[[2]]
test_dat <- split_dat[[1]]

#nrow(train_dat)


# Scale/balance data
weights <- table(train_dat$outcome)
weights <- sum(weights) / (length(weights) * weights)

# Make numeric: year, numDayOfWeek_sin/cos, TimeOfDay_sin/cos, month_sin/cos, Vulnerability, outcome 
train_dat <- train_dat %>%
  mutate(
    year = as.numeric(year),
    numDayOfWeek_sin = as.numeric(numDayOfWeek_sin),
    numDayOfWeek_cos = as.numeric(numDayOfWeek_cos),
    TimeOfDay_sin = as.numeric(TimeOfDay_sin),
    TimeOfDay_cos = as.numeric(TimeOfDay_cos),
    month_sin = as.numeric(month_sin),
    month_cos = as.numeric(month_cos),
    Vulnerability = as.numeric(Vulnerability),
    outcome = as.numeric(outcome)
  )
train_x <- train_dat %>% select(-outcome)

test_dat <- test_dat %>%
  mutate(
    year = as.numeric(year),
    numDayOfWeek_sin = as.numeric(numDayOfWeek_sin),
    numDayOfWeek_cos = as.numeric(numDayOfWeek_cos),
    TimeOfDay_sin = as.numeric(TimeOfDay_sin),
    TimeOfDay_cos = as.numeric(TimeOfDay_cos),
    month_sin = as.numeric(month_sin),
    month_cos = as.numeric(month_cos),
    Vulnerability = as.numeric(Vulnerability),
    outcome = as.numeric(outcome)
  )
test_x <- test_dat %>% select(-outcome)

dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_dat$outcome,
                      weight = weights[as.numeric(train_dat$outcome) + 1])
dtest <- xgb.DMatrix(data = as.matrix(test_x))
```

Let's tune the number of rounds for this model
```{r tune violence}
# Tune number of rounds
tune_nrounds2 <- xgb.cv(data = dtrain,
              nfold = 5,
              eta = 0.1, 
              nrounds = 1000, 
              early_stopping_rounds = 50,
              verbose = 1, 
              nthread = 1,
              print_every_n = 20, 
              objective = "binary:logistic", 
              eval_metric = "error")

best_nrounds_violent <- tune_nrounds2$best_iteration
best_nrounds_violent
```

Now we can run and predict with our model using this ideal number of rounds.
```{r test violence}
# Train the model
violent_model <- xgb.train(data = dtrain, 
                   nrounds = best_nrounds_violent, # best iteration
                   eta = 0.1,
                   objective = "binary:logistic", 
                   eval_metric = "error")

predictions <- predict(violent_model, dtest)
pred_class <- ifelse(predictions > 0.5, 1, 0)
confusionMatrix(factor(pred_class), factor(test_dat$outcome))
```

We see a balanced accuracy of 0.7163 with this model. Our sensitivity score is 0.6528, meaning that our model correctly identifies 65% of non-violent (0) cases as non-violent. Our specificity score is 0.7798, meaning that our model correctly identifies 78% of violent (1) cases as violent.  

Let's look at variable importance.
```{r}
imp_mat <- xgb.importance(model = violent_model)
xgb.plot.importance(imp_mat, top_n = 7)
```

In this model, we see premis code and district number as the two strongest predictors. We can look at the pdp plots for both these factors.

```{r}
pdp_result1 <- partial(violent_model, pred.var = "Premis_Code", plot = TRUE, train = as.matrix(train_x),
                      prob=TRUE)
pdp_result1

pdp_result2 <- partial(violent_model, pred.var = "District_Num", plot = TRUE, train = as.matrix(train_x),
                      prob=TRUE)
pdp_result2
```

Let's see if we can improve our model with threshold testing. We chose 0.5 as our threshold in our first iteration of the model.
```{r}
curr = 0
for (i in seq(0.4, 0.6, by = 0.01)){
  test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > i, 1, 0))
  
  accuracy <- mean(as.numeric(test_pred) == as.numeric(test_dat$outcome))
  conf_matrix <- confusionMatrix(factor(test_pred), factor(test_dat$outcome))
  ba <- conf_matrix$byClass["Balanced Accuracy"]
  
  if (ba > curr){
    best = c(i, ba)
    curr = ba
  }
}

pred_class <- ifelse(predictions > best[1], 1, 0)
confusionMatrix(factor(pred_class), factor(test_dat$outcome))
```

Threshold testing proved that we selected the correct threshold for our model, meaning that we have found the highest balanced accuracy possible.

## Additional Discussion & Conclusions
  For our analysis, we looked at crime data in Los Angeles over a three-year span. We hoped to identify areas of high-crime within the city and then determine the factors that make these areas violent/high-crime. 
  We began by creating 2 derived variables: vulnerability and severity. Vulnerability captured the victim's age and sex, and severity captured the number of crimes and accounted for the severity of each according to the crime code provided. Other important variables were the date/time of the crime (hour, month, year), if there was a weapon used, and if there was an arrest involved.
  We began with PAM Clustering, which identified areas of high-crime based on the number of guns, number of blades, number of arrests, average severity, and average vulnerability. From our PAM Clustering model, we learned the clusters that the Los Angeles police department should prioritize in terms of number of officers and preparation.

  - The districts in cluster 10 are high-crime and violent. Officers in these districts should be prepared with equipment for armed crimes.

  - The districts in clusters 4 and 11 have more vulnerable victims and are high-crime as well. Officers in these districts should be prepared to treat more vulnerable victims.

  - The districts in cluster 1 are the safest for all people (vulnerable/non-vulnerable) to live and are the safest overall. Officers in these districts should be ready to move to and assist those in more high-crime areas.

  We then moved into prediction. These models used the clusters identified as high-crime (4, 10, and 11) for modeling. From our XGBoost models, we learned the most important factors that predict a violent crime and the time of the day that a crime will occur. For both models, we see that premis code is the dominant predictor. In terms of practical conclusions, the Los Angeles police department should be prepared to help those in the areas where Premis Code is highest on the partial dependence plot. 

## Future Work
When considering future work for this project, we would like to expand our analysis to other major cities in the US. It would be interesting to see if other cities had similar patterns of violence and times of day and also if the same values have the same predictive power across the US.

When considering implementation for this project, we would expand our findings by using different weightings to calculate our severity or vulnerability index. Our hope would be to do more research in victim fatality, which would guide us to better quantify the vulnerable population and even include vulnerability due to race. In addition, we did not use any time series models in our prediction, instead transforming our time variables with sin/cos or leaving them as factors. This meant that we were not investigating patterns over time. This would be an interesting analysis to help assess whether our project or any other initiatives in the city are successful, by seeing if crime is trending down.

## Contribution

Imogen Meers & Sarah Deussing

Both have contributed to data cleaning, project description, and analyses.

## Bibliography

“City of Los Angeles - Crime Data from 2020 to Present.” Data.Gov, data.lacity.org, 20 Sept. 2024, catalog.data.gov/dataset/crime-data-from-2020-to-present. 

California Department of Justice. (2024). Law Enforcement Code Tables. Retrieved from https://oag.ca.gov/law/code-tables

Pangarego, R. (2023). Case Study: LAPD Crime Data from 2020 to Oct 2023 - Analysis Using Python. Medium. Retrieved from https://medium.com/@rpangarego/case-study-lapd-crime-data-from-2020-to-oct-2023-analysis-using-python-1d5d6dbf58f8

Singh, N. (n.d.). Crime Data. Kaggle. Retrieved from https://www.kaggle.com/code/navinpalsingh/crime-data/notebook
