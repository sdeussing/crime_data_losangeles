---
title: "ML_Project_Intro"
author: "Sarah Deussing & Imogen Meers"
date: "2024-09-20"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation
Our group is working with Los Angeles crime data from 2021-2024. The dataset is divided into region and lists the date, time, and type of crime. This is an interesting problem because of its practical implications - after identification of areas with high crime, the Los Angeles police department can place more officers at such locations (and at certain identified times).

This problem has been attempted by other people on Kaggle although mostly in an exploratory sense. Many projects on this dataset look at victim type and crime rate by hour, which are both features we will adapt and use for our analysis by including victim age in our derived variables and predicting hours of crime based on other features.

One analysis by Navin Pal Singh on Kaggle, looked into distribution of crimes both geospatial, by time and by crime type. The output of his analysis was mostly visualizations including a bar chart of crime codes, a 3D plot of number of crimes by time and area, and a map plot of the areas. In general, his visualizations were hard to read with many variables and points overlapping on the axis and in the plot. We will look into using a similar technique of maps and bars to see distribution for our exploratory analysis, before choosing how to group our data for our models.

Another analysis by Ronaldo Pangarego, that was published on Medium did a deep dive into all the variables in the LAPD crime data set. He found years, areas and days of the week that had the most crime. As well as distribution of crime type, weapon type and victim. Pangarego found that the most common crime type was "vehicle-stolen" and weapon was "strong-arm" so it will be interesting to see going forward if these categories have more predictive power in any of our models.

Our analysis will be unique in that we are going to introduce several derived variables (explained in the Data Overview section below). We will also be using machine learning models to predict the type and time of crime, rather than producing mostly descriptive outputs.

## Problem Framing
Our proposed solution can be divided into 2 parts.

Part 1:
Our plan is to identify districts of Los Angeles and times of the day that are "high crime." We will do so via clustering methods, and we will account for the severity of the crime and vulnerability of victim in our analysis.

Part 2:
Of the districts identified as "high-crime" in Part 1, we will then predict the next time and type of crime (violent/non-violent) to occur.

## Data Overview
The dataset contains columns that give the date, time, and location (area code, area name, and exact location) of the reported crime. There are also columns for the use of weapons (yes/no), the crime code, and the longitude/latitude coordinates. 

We will create the following __row-wise derived variables__:

1. __Severity__ index that evaluates crime codes and assigns each crime a ranking of how severe the offense was

  - This information will be useful in deciding if an area is "high-crime." For example, if two areas have similar crime rates/totals, the crime severity will differentiate these two areas. 
  
  - By having a final, combined severity it also allows us to evaluate whether two minor crimes are worse than one major crime.
  
  - Upon research, we found the Uniform Crime Reporting User Manual, which gave a break down of how offenses should be classified. It described that a lower crime code indicated a more violent crime and that each crime is either part 1, a crime against person or property, or a less severe part 2 crime, which is any other crime. Using this information, we ordered each crime firstly by part, then by crime code and gave a severity index from 140 (most severe) to 1 to (least severe).
  
  a. Severity for Crime Code 1
  
  b. Severity for Crime Code 2
  
  c. Severity for Crime Code 3
  
  d. Final Severity, which is a weighted severity that takes into account each crime code. Crime code 1 is the primary offense so is 75% of final, crime code 2 is the secondary offesnse so is 25% of final and crime code 3 is the tertiary offense so is 10% of final
  
  
2. __Vulnerability__ index that evaluates age and gender of the victim and assigns a number to decide how vulnerable they would be
  - This is be calculated by 3 points for female and + 1 point for every year < 18 or > 60
  - The introduction of this variable eliminates some of our project's reproducability as it is subjective how much being female or young/old affects your vulnerability.
  
   
3. An indicator variable to describe gun (1) or not (0)

4. An indicator variable to describe blade (1) or not (0)

5. An indicator variable to describe Adult Arrest (1) or not

6. An indicator variable to describe Juv Arrest (1) or not (0)


From our row-wise derived variables, we will create __district-wise derived variables__ for use in clustering:

1. Mean Severity

2. Mean Vulnerability

3. Number of Gun Crimes

4. Number of Blade Crimes

5. Number of Adult Arrests

6. Number of Juvenile Arrests

7. Most common weapon type

8. Most common premises at which the crime occured (e.g. does District X have lots of crimes in family homes?)

9. Mean latitude

10. Mean longitude


The dataset can be accessed at:
https://catalog.data.gov/dataset/crime-data-from-2020-to-present

## Implementation

#### Initial Data Cleaning

```{r Read in Data}
library(readr)
crime <- read_csv("Crime_Data_from_2020_to_Present.csv")
head(crime)
summary(crime)
```


```{r Clean Column Names: Eliminate Spaces}
#colnames(crime)
new_colnames <- c("DR_NO", "Date_Rptd", "Date_Occurred", "Time_Occurred", "Area",
                  "Area_Name","District_Num", "Part_1_2", "Crime_Code", "Crime_Code_Desc",
                  "Mocode", "Vict_Age", "Vict_Sex", "Vict_Descent", "Premis_Code",
                  "Premis_Desc", "Weapon_Used_Code", "Weapon_Desc", "Status", "Status_Desc",
                  "Crm_Cd1", "Crm_Cd2", "Crm_Cd3", "Crm_Cd4", "Location", "Cross_Street",
                  "LAT", "LON")
colnames(crime) <- new_colnames

```

#### Exploratory Analysis

Number of Crimes by Area Code
```{r Number of Crimes by Area Code}
library(ggplot2)
by_area <- ggplot(crime, aes(x=Area)) + geom_bar() +
  labs(title = "Number of Crimes by Area Code",
       x = "Area Code",
       y = "Number of Crimes") +
  theme_minimal()
by_area
```

The areas with the most crime in the dataset are: 1, 3, 12, and 14.

Number of Crimes by Victim Age/Sex
```{r Number of Crimes by Victim Age and Sex}
filtered <-  crime[crime$Vict_Age > 0 & ((crime$Vict_Sex == 'F' | crime$Vict_Sex == 'M') & !is.na(crime$Vict_Sex)),]

by_age <- ggplot(filtered, aes(x=Vict_Age)) + 
  geom_histogram(fill = "grey", color = "black") +
  geom_histogram(data = filtered[filtered$Vict_Age > 0 & filtered$Vict_Age < 18 | filtered$Vict_Age >= 60,], fill = "red", color = 'black', ) + 
  labs(title = "Vulnerability- Number of Crimes by Victim Age and Sex",
       x = "Victim Age",
       y = "Number of Crimes") +
  theme_minimal() +
  annotate("text", x = 80, y = 30000, label = "Vunerable Population\n Age < 18 or Age > 60", color = "red", size = 3) +
  facet_wrap(~Vict_Sex)
by_age
```

The distribution of victim ages is relatively normal and similar between male and female with the most crimes being committed against males and females aged 25 to 50. From the area of each histogram you can see there is a roughly equal number of crimes committed against males and females. 

Severity of Crimes (Area)
```{r Severity of Crimes by Area}
library(dplyr)
library(ggmap)


#Dataset including indicator variables for armed and arrested
indicators <- crime
indicators$Arrest <- ifelse((indicators$Status == 'JA' | indicators$Status == 'AA') & !is.na(indicators$Status), 1, 0)
indicators$Armed <- ifelse(!is.na(indicators$Weapon_Used_Code), 1, 0)


#Summary Table by Area
summary_by_area <- indicators[indicators$LON != 0 & indicators$ LAT !=0,] %>%
  group_by(Area) %>%
  summarise(
    avg_lat = mean(LAT),
    avg_lon = mean(LON),
    Percent_Arrests = (sum(Arrest) / n()) * 100,
    Percent_Armed = (sum(Armed) / n()) * 100
  )

```

Crimes on Los Angeles Map
```{r crimes on LA map}
register_stadiamaps("9e07144d-cdab-48f9-a35d-298e1bbece2e")

# Define the bounding box for Los Angeles
bbox <- c(left = -118.6682, bottom = 33.7037, right = -118, top = 34.3373)

# Get the map
la_map <- get_stadiamap(bbox = bbox, zoom = 12, maptype = "stamen_toner")


# Plot the map
armed_map <- ggmap(la_map) +
  geom_point(data = indicators, aes(x = LON, y = LAT, color = as.factor(Armed), alpha = 0.05)) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  scale_color_manual(
    values = c('1' = "red", '0' = "blue"),
    labels = c("Unarmed", "Armed")
  ) + 
  labs(
    title = "Map of LA showing all Crimes by Weapon Usage",
    color = "Weapon Usage"
  )+
  guides(alpha = "none")

armed_map
```

This shows every crime in out data set coloured by armed or not. It is quite difficult to tell as there are so many crimes that overlap but you can see some areas where there are more concentrated crimes with weapons, such as the centre.

```{r arrest map}
arrest_map <- ggmap(la_map) +
  geom_point(data = summary_by_area, aes(x = avg_lon, y = avg_lat, color = Percent_Arrests, size = 3))+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing % of Crimes ending in Arrest by Area"
  ) +
  guides(size = "none") +
  scale_color_gradient(low = "blue", high = "red") 
#arrest_map


armed_map1 <- ggmap(la_map) +
  geom_point(data = summary_by_area, aes(x = avg_lon, y = avg_lat, color = Percent_Armed, size = 3))+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing % of Armed Crimes by Area"
  ) +
  guides(size = "none") +
  scale_color_gradient(low = "blue", high = "red") 
#armed_map1

library(gridExtra)
grid.arrange(arrest_map, armed_map1, nrow=1)
```

From this map, we can see there are areas particularly towards the north and centre where there are a higher percentage of armed crimes and crimes that lead to arrests. This indicates that both armed and arrested may be good variables to help cluster our areas into high-low crime. 

#### Creating derived variables

Vulnerability Metric
```{r Create Vulnerability Metric}
crime$Vict_Age <- ifelse(crime$Vict_Age <= 0, NA, crime$Vict_Age)
crime$Vulnerability <- 0

 createVuln <- function(row){
   x <-  0
  if(!is.na(row['Vict_Sex'])){ #adding points for F
    if (row['Vict_Sex'] == "F"){
      x  <- x + 3
    }
  }
  
  if(!is.na(row['Vict_Age'])){ #adding points for Age
    if (as.numeric(row['Vict_Age']) < 18){
      x  <- x + 18 - as.numeric(row['Vict_Age'])
    }
    
    else if (as.numeric(row['Vict_Age']) > 60){
      x <- x + as.numeric(row['Vict_Age']) - 60
    }
  }
   return(x)
}
crime$Vulnerability <- apply(crime, 1, createVuln)
```

Severity Metric
```{r Create Severity Metric}
library(dplyr)
#Ranked our crimes in terms of hierarchy and if part 1 or 2 offense
rank_code <- crime %>% select(Crime_Code_Desc, Crime_Code, Part_1_2)%>% distinct(Crime_Code_Desc,Crime_Code, Part_1_2) %>%  arrange(desc(Part_1_2), desc(Crime_Code)) %>%  mutate(Severity = row_number())

#Join on overall CrimeCode --> Do this for Crime Code 1, 2, 3, 4
rank_code <- rank_code %>% select(Severity, Crime_Code)

#Crime Code 1
crime <- left_join(crime, rank_code, by = "Crime_Code")
crime <- crime %>%
  rename(Severity_1 = Severity)

#Crime Code 2
rank_code <- rank_code %>%
  rename(Crm_Cd2 = Crime_Code)
crime <- left_join(crime, rank_code, by = "Crm_Cd2")
crime <- crime %>%
  rename(Severity_2 = Severity)

# Crime Code 3
rank_code <- rank_code %>%
  rename(Crm_Cd3 = Crm_Cd2)
crime <- left_join(crime, rank_code, by = "Crm_Cd3")
crime <- crime %>%
  rename(Severity_3 = Severity)

# Crime Code 4 is all NA, so remove it.
crime <- crime %>%
  select(-c(Crm_Cd4))

# Calculate final severity
#0.75*CRM1 +0.25*CRM2 + 0.1*CRM3 = FINAL SEVERITY
crime$Severity_2[is.na(crime$Severity_2)] <- 0
crime$Severity_3[is.na(crime$Severity_3)] <- 0
crime <- crime %>%
  mutate(Final_Severity = (0.75*Severity_1) + 
  (0.25*Severity_2) + 
  (0.1*Severity_3))
```

Weapons & Arrests
```{r Weapons and arrests}
# Arrests --> count AA (adult arrest) and JA (juvenile arrest)
crime$Adult_Arrest <- ifelse(crime$Status == "AA", 1, 0)
crime$Juv_Arrest <- ifelse(crime$Status == "JA", 1, 0)

# Weapons --> gun (starts with 1), blade (starts with 2)
crime$Weapon_Used_Code <- as.character(crime$Weapon_Used_Code)
crime$Gun <- ifelse(substr(crime$Weapon_Used_Code,1,1) == "1", 1, 0)
crime$Gun <- ifelse(is.na(crime$Gun), 0, crime$Gun)
crime$Blade <- ifelse(substr(crime$Weapon_Used_Code,1,1) == "2", 1, 0)
crime$Blade <- ifelse(is.na(crime$Blade), 0, crime$Blade)
```

Final Crime Dataframe
```{r Create Final Dataframe}
crime <- crime %>%
  select(-c(DR_NO, Date_Rptd, Part_1_2, Mocode, Location, Cross_Street, Vict_Age, Vict_Sex,
            Crm_Cd1, Crm_Cd2, Crm_Cd3, Vict_Descent, Severity_1, Severity_2, Severity_3))
#write.csv(crime, 'crime_final.csv')
```

Group Crimes by District
```{r Group by District}
library(DescTools)
crime$Weapon_Used_Code <- as.numeric(crime$Weapon_Used_Code)

districts <- crime %>% group_by(District_Num) %>%
  summarize(Most_Common_Premis = Mode(Premis_Code, na.rm=TRUE),
            Most_Common_Crime = Mode(Crime_Code, na.rm=TRUE),
            Avg_Vulnerability = mean(Vulnerability),
            Avg_Severity = mean(Final_Severity),
            Avg_LON = mean(LON),
            Avg_LAT = mean(LAT),
            Num_Adult_Arrest = sum(Adult_Arrest),
            Num_Juv_Arrest = sum(Juv_Arrest),
            Num_Gun = sum(Gun),
            Num_Blade = sum(Blade),
            Most_Common_Weapon = Mode(Weapon_Used_Code, na.rm=TRUE))

write.csv(districts, 'districts.csv')
```

Read in dataframe (from above)
```{r, eval = FALSE}
#districts <- read.csv('districts.csv')
#crime <- read.csv('crime_final.csv')
```


#### PAM K-Means Clustering
```{r PAM K-Means CLustering}
library(cluster)
library(tibble)
library(ggplot2)

d <-  districts
clust_data <- ungroup(d) %>% select(-c(Avg_LON, Avg_LAT, District_Num))

#factor non numeric
clust_data$Most_Common_Premis <- as.factor(clust_data$Most_Common_Premis)
clust_data$Most_Common_Crime <- as.factor(clust_data$Most_Common_Crime)
clust_data$Most_Common_Weapon <- as.factor(clust_data$Most_Common_Weapon)


#scale numeric cols
clust_data <- clust_data %>%
    mutate(across(where(is.numeric), scale))


clust_data <- clust_data  %>% 
  mutate(across(where(is.matrix), as.numeric))


gower_dist <- 
  clust_data %>% daisy(metric = "gower")

sil_width <- c(NA)
for (i in 2:20){
  pam_fit <- pam(gower_dist, diss =TRUE, k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}


sil_width %>% 
  as_tibble() %>% 
   rowid_to_column() %>% 
   filter(rowid %in% c(2:20)) %>% 
   ggplot(aes(rowid, value)) +
   geom_line(colour  = 'black', size = 0.7) +
   geom_point(colour = 'black', size = 1.3) +
   theme_minimal() +
   labs(title = 'Silhouette Widths of k-medoid Clusters',
        x     = "Number of clusters",
        y     = 'Silhouette Width') +
   theme(plot.title = element_text(hjust = 0.5))

#sil_width %>% 
 # as_tibble() %>% 
  # rowid_to_column()
```
Looking at this graph, both 7 and 13 are local maximums so we will look at the clustering for both of these.

```{r}
library(Rtsne)

pam_fit7 <- pam(gower_dist, diss =TRUE, k = 7)
pam_fit13 <- pam(gower_dist, diss =TRUE, k = 13)



tsne_obj7 <- Rtsne(gower_dist, is_distance = TRUE)
tsne_obj7$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit7$clustering)) %>% 
  # plot
  ggplot(aes(x = X, y = Y, colour = cluster)) +
  geom_point()  +
  theme_light() +
  labs(title     = 't-SNE 2D Projections of 7-medoid Clusters')  +
  theme(plot.title = element_text(hjust = 0.5))



tsne_obj13 <- Rtsne(gower_dist, is_distance = TRUE)
tsne_obj13$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit13$clustering)) %>% 
  # plot
  ggplot(aes(x = X, y = Y, colour = cluster)) +
  geom_point()  +
  theme_light() +
  labs(title     = 't-SNE 2D Projections of 13-medoid Clusters')  +
  theme(plot.title = element_text(hjust = 0.5))
```

The clusters when k = 13 are more distinct than when k = 7, so we will use 13 clusters for further analysis.
```{r}
library(tidyr)
# Extract clusters
clusters_13 <- pam_fit13$cluster
# Extract centers
#centers_13 <- pam_fit13$medoids
temp <- cbind.data.frame(clust_data, clusters_13)
centers <- temp %>% 
  group_by(clusters_13) %>%
  summarise()

res_mat <- as.data.frame(matrix(NA, nrow = length(unique(clusters_13)), ncol = 6))

for(i in 1:nrow(res_mat)){
  res_mat[i,] <- colMeans(clust_data[which(clusters_13 == i), c(3:8) ], na.rm  =TRUE)
}

names(res_mat) <- names(clust_data)[c(3:8)]
# Create cluster vector
cluster <- c(1:13)
# Join cluster vector and centers
center_df <- data.frame(cluster, res_mat)

# Reshape the data
center_reshape <- gather(center_df, features, values, Avg_Vulnerability:Num_Blade)
# View result
head(center_reshape)

# Create plot
g_heat_2 <- ggplot(data = center_reshape, # Set dataset
                   aes(x = features, y = cluster, fill = values)) + # Set aesthetics
  scale_y_continuous(breaks = seq(1, 13, by = 1)) + # Set y axis breaks
  geom_tile() + # Set geom tile for heatmap
  coord_equal() +  # Set coord equal 
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =0, # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  coord_flip() # Rotate plot
# Generate plot
g_heat_2
```

Group 10- Lots of guns/blade/arrests
Group 11- High vulnerability and severity, low guns/blades/arrests
Group 4- High vulnerability
Group 1- safest (low severity/vulnerability, guns/blades/arrests)

Assign cluster numbers back for mapping purposes.
```{r}
d_details <- d %>% select(District_Num, Avg_LON, Avg_LAT)
length(clusters_13)
named_clust <- cbind(d_details, clusters_13) %>% rename( "Cluster_Num" = `...4`)

categoric_added <- cbind(named_clust, d[,c(2, 3, 12)])  
categoric_added <- categoric_added[categoric_added$Cluster_Num %in% c(1, 4, 10, 11),]

sum_by_cluster <- categoric_added %>% 
  group_by(Cluster_Num) %>%
  #select(-District_Num) %>%
  mutate(modePrem = Mode(Most_Common_Premis),
         modeCrime = Mode(Most_Common_Crime),
         modeWeapon = Mode(Most_Common_Weapon),
         avgLat = mean(Avg_LAT),
         avgLon = mean(Avg_LON))
```

Let's look at our clusters on a map.
```{r Clusters by Color Map}

cluster_map <- ggmap(la_map) +
  geom_point(data = sum_by_cluster, aes(x = Avg_LON, y = Avg_LAT, color = as.factor(Cluster_Num)), size = 2)+
  theme(
    plot.title = element_text(size = 10),  # Adjust the size here
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
  ) +
  labs(
    title = "Map of LA showing our Highlighted Clusters",
    color = "Cluster Number"
  ) +
  guides(size = "none") 
cluster_map
```


#### Prediction

Our clusters of interest are 1, 4, 10, and 11.
  - Cluster 10: lots of guns, blades, and arrests
  - Cluster 11: high vulnerability and severity measures, but low weapon use and arrests
  - Cluster 4: high vulnerability measures
  - Cluster 1: the safest cluster (low vulnerability, severity, weapon use, arrests)

In order to analyze these clusters, we'll assign the cluster number back to the entries in our original crime dataframe and then subset clusters of interest.
```{r Put cluster numbers back}
clust_crime <- crime %>%
  left_join(named_clust, by = "District_Num")
```


We want to analyze trends within 'violent crimes.' For our analysis, we will consider a violent crime one that has a crime code of less than 200 (making it the most severe in the Los Angeles rankings), or uses either a gun or a blade.
```{r Make 'violent crime' variable}
# violent crime = crime code < 200 or uses a blade/gun
clust_crime$violent <- ifelse (((clust_crime$Crime_Code < 200) | (clust_crime$Gun == 1) | (clust_crime$Blade == 1)), 1, 0)
```

Looking further into this violence measure, we can analyze the most violent districts within each "high-crime" cluster. This information could provide the Los Angeles police department with the most high-crime districts within these areas. 
```{r Find most violent districts}
worst10 <- clust_crime %>% 
  filter(Cluster_Num == 10) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst10$District_Num[1] # District 1249 is the most "violent" in cluster 10

worst11 <- clust_crime %>% 
  filter(Cluster_Num == 11) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst1$District_Num[1] # District ___ is the most "violent" in cluster 11

worst4 <- clust_crime %>% 
  filter(Cluster_Num == 4) %>%
  group_by(District_Num) %>% 
  mutate(sum_violent = sum(violent)) %>%
  arrange(desc(sum_violent)) %>%
  select(District_Num) %>%
  head(1)
worst1$District_Num[1] # District __ is the most "violent" in cluster 4

```


For further analysis, we will look at all the districts within our three highlighted "high-crime" clusters: 4, 10, and 11.

Further setup for our XGBoost models include standardizing time and date of the crime. We will also create a variable for day of the week Sunday (1) - Saturday (7).
```{r Clean Time/Date}
library(lubridate)
clust_crime$hour <- as.numeric(clust_crime$Time_Occurred)  %/% 100

clust_crime$Date <- as.Date(mdy_hms(clust_crime$Date_Occurred))
clust_crime$DayOfWeek <- wday(mdy_hms(clust_crime$Date_Occurred), label = TRUE)
clust_crime$month <- month(mdy_hms(clust_crime$Date_Occurred))
clust_crime$year <- year(mdy_hms(clust_crime$Date_Occurred))


day_map <- c(Sun = 1, Mon = 2, Tues = 3, Wed = 4, Thurs = 5, Fri = 6, Sat = 7)
clust_crime$numDayOfWeek <- day_map[clust_crime$DayOfWeek]
```

###### Hour (Time of Day) Models
Our first prediction will be the hour that a crime occurs. Factors will include whether the crime was violent, the crime code, premise code, and whether there were any arrests or weapons involved in the crime.
```{r XGBoost on districts}
library(xgboost)
library(caret)
set.seed(1234567)


vars <- clust_crime %>%
  select(violent, Crime_Code, Premis_Code, Adult_Arrest, Juv_Arrest, Gun, Blade)
outcome <- clust_crime$hour

train_indices <- sample(1:nrow(clust_crime), 0.8 * nrow(clust_crime))
train_set <- vars[train_indices, ]
test_set <- vars[-train_indices, ]
train_label <- outcome[train_indices]
test_label <- outcome[-train_indices]

dtrain <- xgb.DMatrix(data = as.matrix(train_set), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_set))

# Train the model
hour_model <- xgb.train(data = dtrain, 
                   nrounds = 100,
                   eta = 0.1,
                   objective = "multi:softmax",
                   num_class = 24,
                   eval_metric = "merror")

predictions <- predict(hour_model, dtest)
pred_and_test <- cbind.data.frame(predictions, test_label)
confusionMatrix(factor(predictions), factor(test_label))
```

This model is too granular to make good predictions. Instead, let's predict time of day in 6 four-hour periods.
  - 6am - 9:59am (morning = 0)
  - 10am - 1:59pm (midday = 1)
  - 2pm - 5:59pm (afternoon = 2)
  - 6pm - 9:59pm (evening = 3)
  - 10pm - 1:59am (late night = 4)
  - 2am - 5:59am (early morning = 5)
  
These periods were assigned to capture 'night time' (10pm - 6am) as sequential periods.
```{r Make time periods}
time_of_day_func <- function(hour) {
  if (hour >= 6 && hour < 10) {
    return(0)  # morning
  } else if (hour >= 10 && hour < 14) {
    return(1)  # midday
  } else if (hour >= 14 && hour < 18) {
    return(2)  # afternoon
  } else if (hour >= 18 && hour < 22) {
    return(3)  # evening
  } else if (hour >= 22 || hour < 2) {
    return(4)  # late night
  } else {
    return(5)  # early morning
  }
}

clust_crime <- clust_crime %>%
  mutate(TimeOfDay = sapply(hour, time_of_day_func))
```

Now we can run a model to predict a crime's time period of the day.
We will do so with our 'worst clusters' - 2, 10, and 11.
```{r XGBoost for time period}
worst_clusts <- clust_crime %>%
  filter(Cluster_Num == 4 | Cluster_Num == 10 | Cluster_Num == 11)

worst_clusts$District_Num <- as.numeric(worst_clusts$District_Num)

vars <- worst_clusts %>%
  select(Crime_Code, District_Num, Premis_Code, Adult_Arrest, Juv_Arrest, Gun, Blade)
outcome <-worst_clusts$TimeOfDay

train_indices <- sample(1:nrow(worst_clusts), 0.8 * nrow(worst_clusts))
train_set <- vars[train_indices, ]
test_set <- vars[-train_indices, ]
train_label <- outcome[train_indices]
test_label <- outcome[-train_indices]

dtrain <- xgb.DMatrix(data = as.matrix(train_set), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_set))
```

Let's tune the number of rounds for this model.
```{r tune for time period}
# Tune number of rounds
tune_nrounds <- xgb.cv(data = dtrain,
              nfold = 5,
              eta = 0.1, 
              nrounds = 1000, 
              early_stopping_rounds = 50,
              verbose = 1, 
              nthread = 1,
              print_every_n = 20, 
              objective = "multi:softprob", 
              num_class = 6,
              eval_metric = "merror")
```

The best number of rounds is: _____

Now we can train and predict with our model.
```{r}
# Train the model
tod_model <- xgb.train(data = dtrain, 
                   nrounds = 59, # best iteration
                   eta = 0.1,
                   objective = "multi:softprob",
                   num_class = 6,
                   eval_metric = "merror")

predictions <- predict(tod_model, dtest)
pred_and_test <- cbind.data.frame(predictions, test_label)
confusionMatrix(factor(predictions), factor(test_label))
```

Let's look at variable importance.
```{r}
# Extract importance
imp_mat <- xgb.importance(model = tod_model)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 7)

imp_mat %>%
  arrange(desc(Gain))
```

We can make partial dependence (pdp) plots for our most important variables.
```{r}
library(pdp)

pdp_result <- partial(tod_model, pred.var = "Crime_Code", plot = TRUE, train = as.matrix(train_set),
                      prob=TRUE)
pdp_result
```

```{r}
library("ggforce")
library("lightgbm")
library("SHAPforxgboost")

shap_long_iris <- shap.prep(xgb_model = tod_model, X_train = as.matrix(train_set))

# Plot SHAP importance
shap.plot.summary(shap_long_iris, scientific = TRUE)
```


###### Violence Model
In our next model, we will predict whether a crime will be violent. To do so, our predictors will include the day of the week, hour of the day, premise code, area, and the vulnerability level of the individual to which the crime is performed.

This will help an officer know if a crime is reported at on x day at x time in x premises with x victim, whether they should be prepared for violence.

First, we can encode hour as a sine/cosine value to make it loop (i.e. 00:00 is after 23:59).
```{r Sine/Cos Encoding}
encode <- function(data, idx, max_val){
  print(colnames(data[,idx]))
  col_sin <- paste0(colnames(data[,idx]), '_sin')
  
  col_cos <- paste0(colnames(data[,idx]), '_cos')

  sin_func <- function(x) {sin(2 * pi * as.numeric(x)/max_val)}
  cos_func <- function(x) {cos(2 * pi * as.numeric(x)/max_val)}

  
  data$V1 <-  lapply(data[,idx], sin_func)
  data$V2  <-  lapply(data[,idx], cos_func)
  data <- data %>% rename(!!col_sin := V1, !!col_cos := V2)
  return(data)
}

# Encode Day of Week
worst_clusts <- encode(worst_clusts, 31, 7)

#Encode hour
worst_clusts <- encode(worst_clusts, 32, 6)

#Encode month
worst_clusts <- encode(worst_clusts, 29, 12)

```

We will run our violent crime model on the same 'worst clusters' as before.
```{r Violent crime model}
library(splitstackshape)
library(fastDummies)
set.seed(123456)

#make premis and district num dummy variables
worst_clusts <- fastDummies::dummy_cols(worst_clusts,select_columns = c('Premis_Code', 'District_Num'), remove_selected_columns = TRUE )

worst_clusts <- na.omit(worst_clusts)


vars <- worst_clusts %>%
  select(numDayOfWeek, hour, month, year, Premis_Code, District_Num, Vulnerability)
outcome <- worst_clusts$violent
xg_data <- cbind(vars, outcome)

# Perform stratified sampling
 split_dat <- stratified(xg_data, # Set dataset
                         group = c("outcome", "numDayOfWeek", "hour", "month", "year", "Premis_Code", "District_Num"), # Set variables to use for stratification
                         size = 0.2,  # Set size of test set
                         bothSets = TRUE ) # Return both training and test sets
 
 # Extract train data
 train_dat <- split_dat[[2]]
 # Extract test data
 test_dat <- split_dat[[1]]

# Check size
nrow(train_dat)
# Scale/balance data
weights <- table(train_dat$outcome)
weights <- sum(weights) / (length(weights) * weights)

train_dat$numDayOfWeek <- as.numeric(train_dat$numDayOfWeek)
train_dat$hour <- as.numeric(train_dat$hour)
train_dat$month <- as.numeric(train_dat$month)
train_dat$Premis_Code <- as.numeric(train_dat$Premis_Code)
train_dat$District_Num <- as.numeric(train_dat$District_Num)
train_dat$outcome <- as.numeric(train_dat$outcome)

train_x <- train_dat %>% select(-outcome)


test_dat$numDayOfWeek <- as.numeric(test_dat$numDayOfWeek)
test_dat$hour <- as.numeric(test_dat$hour)
test_dat$month <- as.numeric(test_dat$month)
test_dat$Premis_Code <- as.numeric(test_dat$Premis_Code)
test_dat$District_Num <- as.numeric(test_dat$District_Num)
test_dat$outcome <- as.numeric(test_dat$outcome)


test_x <- test_dat %>% select(-outcome)

dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_dat$outcome,
                      weight = weights[as.numeric(train_dat$outcome) + 1])
dtest <- xgb.DMatrix(data = as.matrix(test_x))
```

```{r tune violence}
# Tune number of rounds
tune_nrounds <- xgb.cv(data = dtrain,
              nfold = 5,
              eta = 0.1, 
              nrounds = 1000, 
              early_stopping_rounds = 50,
              verbose = 1, 
              nthread = 1,
              print_every_n = 20, 
              objective = "binary:logistic", 
              eval_metric = "error")
```

```{r test violence}
# Train the model
violent_model <- xgb.train(data = dtrain, 
                   nrounds = 433, # best iteration
                   eta = 0.1,
                   objective = "binary:logistic", 
                   eval_metric = "error")

predictions <- predict(violent_model, dtest)
pred_class <- ifelse(predictions > 0.5, 1, 0)
conf_matrix <- confusionMatrix(factor(pred_class), factor(test_dat$outcome))
```

Let's look at variable importance.
```{r}
# Extract importance
imp_mat <- xgb.importance(model = violent_model)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 7)
```

Let's look at the pdp result for the most important factors.
```{r}
pdp_result <- partial(violent_model, pred.var = "Premis_Code", plot = TRUE, train = as.matrix(train_x),
                      prob=TRUE)
pdp_result
```

In this model using 0.5 as the threshold, we see a balanced accuracy of 0.6576. We see very similar sensitivity and specificity values, meaning that our model is not skewed toward predicting one class over another.

When looking at variable importance, we can see the most important variables in this model are

```{r}
#Threshold testing
curr = 0
for (i in seq(0.4, 0.6, by = 0.01)){
  test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > i, 1, 0))
  
  accuracy <- mean(as.numeric(test_pred) == as.numeric(test_dat$outcome))
  conf_matrix <- confusionMatrix(factor(test_pred), factor(test_dat$outcome))
  ba <- conf_matrix$byClass["Balanced Accuracy"]
  
  
  if (ba > curr){
    best = c(i, ba)
    curr = ba
  }
  
}


pred_class <- ifelse(predictions > best[1], 1, 0)
confusionMatrix(factor(pred_class), factor(test_dat$outcome))
```

By using threshold testing for this model, we improved our balanced accuracy to 0.7250. We see very similar sensitivity and specificity values, meaning that our model is not skewed toward predicting one class over another.

## Results

## Discussion

## Conclusion 

##Future Work

When considering future work for this project, we would like to expand our analysis to other major cities in the US. It would be interesting to see if other cities had similar patterns of violence and times of day and also if the same values have the same predictive power across the US.

When considering implementation for this project, we would expand our findings by using different weightings to calculate our severity or vulnerability index. Our hope would be to do more research in victim fatality, which would guide us to better quantify the vulnerable population and even include vulnerability due to race. In addition, we did not use any time series models in our prediction, instead transforming our time variables with sin/cos or leaving them as factors. This meant that we were not investigating patterns over time. This would be an interesting analysis to help assess whether our project or any other initiatives in the city are successful, by seeing if crime is trending down.

## Contribution

Imogen Meers & Sarah Deussing

Both have contributed to data cleaning, project description, and analyses.

## Bibliography

“City of Los Angeles - Crime Data from 2020 to Present.” Data.Gov, data.lacity.org, 20 Sept. 2024, catalog.data.gov/dataset/crime-data-from-2020-to-present. 

California Department of Justice. (2024). Law Enforcement Code Tables. Retrieved from https://oag.ca.gov/law/code-tables

Pangarego, R. (2023). Case Study: LAPD Crime Data from 2020 to Oct 2023 - Analysis Using Python. Medium. Retrieved from https://medium.com/@rpangarego/case-study-lapd-crime-data-from-2020-to-oct-2023-analysis-using-python-1d5d6dbf58f8

Singh, N. (n.d.). Crime Data. Kaggle. Retrieved from https://www.kaggle.com/code/navinpalsingh/crime-data/notebook
